{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, Tuple\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import block_diag\n",
    "from case_studies import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(f, df, x, p, alpha):\n",
    "    c1 = 0.05\n",
    "    rho = 0.5\n",
    "    iters = 0\n",
    "    max_iters = 200\n",
    "    while f(x + alpha * p) > f(x) + c1 * alpha * np.dot(df(x), p):\n",
    "        alpha *= rho\n",
    "        iters += 1\n",
    "        if iters > max_iters:\n",
    "            raise ValueError(\n",
    "                f\"Backtracking line search did not converge within {max_iters} iterations\"\n",
    "            )\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_steepest_descent(\n",
    "    f: Callable,\n",
    "    df: Callable,\n",
    "    Hf: Callable,\n",
    "    x0: np.ndarray,\n",
    "    A: np.ndarray,\n",
    "    b: np.ndarray,\n",
    "    tol: float,\n",
    "    max_iter: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    rho = 0.5\n",
    "    beta = 1.0\n",
    "    M = np.eye(len(x0)) - A.T @ np.linalg.inv(A @ A.T) @ A\n",
    "    xs = [x0]\n",
    "    grad_norms = [np.linalg.norm(df(x0))]\n",
    "    alphas = [beta]\n",
    "    x = x0\n",
    "    k = 0\n",
    "\n",
    "    # iterate until stopping criterion is met or max_iter is reached\n",
    "    while grad_norms[-1] > tol and k < max_iter:\n",
    "        # calculate gradient to find p_k\n",
    "        p = -M @ df(x)\n",
    "        # find alpha_k using backtracking line search\n",
    "        try:\n",
    "            alpha = backtracking_line_search(f, df, x, p, beta)\n",
    "            alphas.append(alpha)\n",
    "        except ValueError as _:\n",
    "            return np.array(xs), np.array(grad_norms), np.array(alphas)\n",
    "\n",
    "        # calculate x_k+1 and append to xs\n",
    "        x = x + alpha * p\n",
    "        xs.append(x)\n",
    "        # calculate gradient norm and append to grad_norms\n",
    "        grad_norms.append(np.linalg.norm(df(x)))\n",
    "        # update beta_k+1\n",
    "        beta = alpha / rho\n",
    "        # update k\n",
    "        k += 1\n",
    "\n",
    "    return np.array(xs), np.array(grad_norms), np.array(alphas)\n",
    "\n",
    "\n",
    "# Example usage, assuming f, df, A, and b are defined and suitable for the problem\n",
    "# result = linearly_equality_constrained_steepest_descent(f, df, x0, A, b, tol, max_iter, c1, rho)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_newton(\n",
    "    f: Callable,\n",
    "    df: Callable,\n",
    "    hf: Callable,\n",
    "    A: np.ndarray,\n",
    "    b: np.ndarray,\n",
    "    x0: np.ndarray,\n",
    "    tol: float,\n",
    "    max_iter: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "    xs = [x0]\n",
    "    grad_norms = [np.linalg.norm(df(x0))]\n",
    "    x = x0\n",
    "    k = 0\n",
    "    alphas = [1]\n",
    "\n",
    "    # iterate until stopping criterion is met or max_iter is reached\n",
    "    while grad_norms[-1] > tol and k < max_iter:\n",
    "        # calculate hessian at x_k\n",
    "        hf_x = hf(x)\n",
    "\n",
    "        # Check if hessian is positive definite\n",
    "        if np.all(np.linalg.eigvals(hf_x) > 0):\n",
    "            B = hf_x\n",
    "        else:\n",
    "            # Compute eigenvectors and eigenvalues\n",
    "            eigvals, eigvecs = np.linalg.eig(hf_x)\n",
    "            # Modify the Hessian to be positive definite\n",
    "            B = sum(\n",
    "                abs(lam) * np.outer(v, v) for lam, v in zip(eigvals, eigvecs))\n",
    "\n",
    "        # Solve the KKT system for p_k and lambda*\n",
    "        KKT_matrix = np.block([[B, A.T],\n",
    "                               [A, np.zeros((A.shape[0], A.shape[0]))]])\n",
    "        KKT_rhs = np.block([-df(x), np.zeros(b.shape[0])])\n",
    "\n",
    "        solution = np.linalg.solve(KKT_matrix, KKT_rhs)\n",
    "        p_k = solution[:x0.size]\n",
    "        lambda_star = solution[x0.size:]\n",
    "\n",
    "        # find alpha_k using backtracking line search\n",
    "        try:\n",
    "            alpha_k = backtracking_line_search(f, df, x, p_k, 1.0)\n",
    "            alphas.append(alpha_k)\n",
    "        except ValueError as _:\n",
    "            return np.array(xs), np.array(grad_norms), np.array(alphas)\n",
    "\n",
    "        # calculate x_k+1 and append to xs\n",
    "        x = x + alpha_k * p_k\n",
    "        xs.append(x)\n",
    "\n",
    "        # calculate gradient norm and append to grad_norms\n",
    "        grad_norms.append(np.linalg.norm(df(x)))\n",
    "\n",
    "        # update k\n",
    "        k = k + 1\n",
    "\n",
    "    return np.array(xs), np.array(grad_norms), np.array(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value: 3.8946825420201328\n",
      "Optimal x: [-0.10397654 -0.49151622  0.56780412 -0.28817515 -0.12555941 -0.10500461\n",
      "  0.03928466 -0.00578652 -0.00581176 -0.00279835]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "m = 5  # rank of A\n",
    "n = 10  # size of A is m*n\n",
    "\n",
    "# Generate a random matrix A of size m*n\n",
    "A = np.random.rand(m, n)\n",
    "\n",
    "# Use SVD to decompose A, and then reconstruct it with the desired rank of m\n",
    "U, S, VT = np.linalg.svd(A, full_matrices=False)\n",
    "S = np.diag(S)\n",
    "A_rank_m = U @ S @ VT\n",
    "\n",
    "# Check the rank of the generated matrix A\n",
    "rank_A = np.linalg.matrix_rank(A_rank_m)\n",
    "\n",
    "# Generate a random vector b of length m\n",
    "b = np.random.rand(m)\n",
    "\n",
    "# To find x such that Ax + b = 0, we solve the linear system Ax = -b\n",
    "# Since A may not be square, we use the least squares solution\n",
    "x, residuals, rank, s = np.linalg.lstsq(A_rank_m, -b, rcond=None)\n",
    "\n",
    "\n",
    "def constraint(x):\n",
    "    return A @ x + b\n",
    "\n",
    "\n",
    "# Convert the constraint to the form expected by scipy\n",
    "con = {'type': 'eq', 'fun': constraint}\n",
    "\n",
    "result = minimize(f1, x, jac=df1, constraints=con, method='SLSQP')\n",
    "\n",
    "# Check if the optimization was successful\n",
    "if result.success:\n",
    "    # The optimal value under the constraint is found\n",
    "    optimal_x = result.x\n",
    "    optimal_value = result.fun\n",
    "    print('Optimal value:', optimal_value)\n",
    "    print('Optimal x:', optimal_x)\n",
    "else:\n",
    "    # Optimization failed\n",
    "    print('Optimization was not successful. Message:', result.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.894682545427036"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, grad_norms, alphas = constrained_steepest_descent(f1, df1, Hf1, x, A, b,\n",
    "                                                      1e-6, 1000)\n",
    "f1(xs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8946825454144918"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, grad_norms, alphas = constrained_newton(f1, df1, Hf1, A, b, x, 1e-6, 1000)\n",
    "f1(xs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "m = 5  # rank of A\n",
    "n = 10  # size of A is m*n\n",
    "\n",
    "\n",
    "def generate_x(m, n):\n",
    "    # Generate a random matrix A of size m*n\n",
    "    A = np.random.rand(m, n)\n",
    "\n",
    "    # Use SVD to decompose A, and then reconstruct it with the desired rank of m\n",
    "    U, S, VT = np.linalg.svd(A, full_matrices=False)\n",
    "    S = np.diag(S)\n",
    "    A_rank_m = U @ S @ VT\n",
    "\n",
    "    # Check the rank of the generated matrix A\n",
    "    rank_A = np.linalg.matrix_rank(A_rank_m)\n",
    "\n",
    "    # Generate a random vector b of length m\n",
    "    b = np.random.rand(m)\n",
    "\n",
    "    # To find x such that Ax + b = 0, we solve the linear system Ax = -b\n",
    "    # Since A may not be square, we use the least squares solution\n",
    "    x, residuals, rank, s = np.linalg.lstsq(A_rank_m, -b, rcond=None)\n",
    "    return A, b, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
